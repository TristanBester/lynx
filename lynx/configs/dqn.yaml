experiment:
  seed: 0
  total_steps: 1_000_000 # Total number of steps to train for
eval:
  num_episodes: 128
  desired_steps_per_eval: 10_000
train:
  envs_per_device: 1024  
  updates_per_device: 1 # Number of gradient updates per device
  rollout_length: 10 # Number of steps in a rollout per vectorised environment
  updates_per_epoch: 20 # Number of gradients updates per epoch
  epochs_per_eval: 100 # Number of epochs between evaluations
  warmup_steps: 100 # Number of environment steps to take before training starts
  buffer_size: 1_000_000 # Size of the replay buffer 
  batch_size: 512 # Batch size used by each device 
  learning_rate: 5e-4
  tau: 0.005
  gamma: 0.99 
  max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update
  training_epsilon: 0.1 # Epsilon used for training
  max_abs_reward: 1000.0
  huber_loss_parameter: 0.0
network:
  encoder:
    _target_: lynx.networks.encoder.AgentViewEncoder
  backbone:
    _target_: lynx.networks.backbone.MLPBackbone 
    layer_sizes: [256, 256]
  head:
    _target_: lynx.networks.heads.QNetworkHead 
environment:
  # name: Snake-v1
  # observation_attribute: grid 
  # kwargs:
  #   num_rows: 6
  #   num_cols: 6 
  # agent_view_transformer:
  #   _target_: lynx.envs.wrappers.jumanji.transformers.snake.SnakeAgentViewTransformer
  name: SlidingTilePuzzle-v0
  agent_view_transformer:
    _target_: lynx.envs.wrappers.jumanji.transformers.puzzle.PuzzleAgentViewTransformer
defaults:  
  - _self_  
  - override hydra/hydra_logging: disabled  
  - override hydra/job_logging: disabled  

hydra:  
  output_subdir: null  
  run:  
    dir: .
